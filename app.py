import streamlit as st
import warnings
warnings.filterwarnings("ignore")

# Import our visualizer class
from simple_attention import SimpleAttentionVisualizer

# Configure the page
st.set_page_config(
    page_title="üéØ Andaazaa Apna Apna",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Title and description
st.title("üéØ Andaazaa Apna Apna")

st.image("Andaazaa-poster.png", 
         caption="When AI goes W.A.I.L.D! ü§ñ", 
         use_container_width=True, width=600)

st.markdown("""
### Understanding Transformer "Andaazaa" (Approximation)
Andaazaa is an Urdu word that means approximation, which is essentially what transformer models do. See how they perform their approximation by visualizing attention weights.

**How to use:**
1. Enter an English sentence
2. Choose a layer and head to visualize
3. Click "Analyze Attention" to see the magic! ‚ú®
""")

# Create sidebar controls
st.sidebar.header("üéõÔ∏è Controls")

# Input text
source_text = st.sidebar.text_input(
    "Enter English text:",
    value="The cat sat on the mat.",
    max_chars=100,  # Limit for web performance
    help="Keep sentences under 15 words for best performance"
)

# Validate input length
if len(source_text.split()) > 15:
    st.sidebar.warning("‚ö†Ô∏è Please use sentences under 15 words for better performance on web")

# Model controls
layer = st.sidebar.selectbox(
    "Select Layer:",
    options=[0, 1, 2, 3, 4, 5],
    index=2,
    help="Middle layers (2-3) often show the clearest patterns"
)

visualization_type = st.sidebar.radio(
    "Choose Visualization:",
    ["Single Head View", "Multi-Head Comparison"],
    help="Single head is clearer, multi-head shows specialization"
)
analyze_button = st.sidebar.button("üîç Analyze Attention", type="primary", use_container_width=True)

# Add examples
st.sidebar.markdown("### üí° Try These Examples:")
example_sentences = [
    "The cat sat on the mat.",
    "I love machine learning.",
    "Hello world!",
    "The quick brown fox jumps.",
    "How are you today?"
]

for sentence in example_sentences:
    if st.sidebar.button(f"üìù {sentence[:30]}..."):
        source_text = sentence
        st.rerun()

# Main content area
col1, col2 = st.columns([2, 1])

with col1:
    # Main visualization button
    if analyze_button:
        if not source_text.strip():
            st.error("Please enter some English text to analyze!")
        elif len(source_text.split()) > 15:
            st.error("Please use sentences under 15 words for better web performance!")
        else:
            try:
                # Show loading spinner
                with st.spinner("ü§ñ Loading model and generating visualization..."):
                    
                    # Initialize visualizer with caching
                    @st.cache_resource
                    def load_visualizer():
                        return SimpleAttentionVisualizer()
                    
                    viz = load_visualizer()
                    
                    if visualization_type == "Single Head View":
                        # Generate single head visualization
                        translation = viz.visualize_core_attention(
                            source_text, 
                            layer=layer
                        )
                        
                        # Display the plot in Streamlit
                        st.pyplot(fig=None, clear_figure=True)
                        
                        # Show results
                        st.success(f"**Translation:** {translation}")
                        
                    else:
                        # Generate multi-head comparison
                        translation = viz.compare_heads(
                            source_text,
                            layer=layer
                        )
                        
                        # Display the plot
                        st.pyplot(fig=None, clear_figure=True)
                        
                        # Show results
                        st.success(f"**Translation:** {translation}")
                        
                    # Add interpretation guide
                    st.markdown("""
                    ### üìñ How to Read the Visualization:
                    - **Rows (Y-axis):** German words being generated
                    - **Columns (X-axis):** English words from input
                    - **Dark blue:** High attention (model focusing here)
                    - **Light blue:** Low attention (model ignoring)
                    - **Diagonal patterns:** Direct word-to-word translation
                    - **Scattered patterns:** Contextual/grammatical relationships
                    """)
                    
            except Exception as e:
                st.error(f"Error: {str(e)}")
                st.error("Try a simpler sentence or check your internet connection.")

with col2:
    # Information panel
    st.markdown("""
    ### üéì What You're Learning:
    
    **"Andaazaa" (Approximation)** is how transformers work:
    
    ‚úÖ **Pattern Recognition**: Models learn patterns from millions of examples
    
    ‚úÖ **Attention Weights**: Show which words are important for translation
    
    ‚úÖ **No Perfect Rules**: Just very good approximations based on data
    
    ‚úÖ **Context Matters**: Same word can have different "Andaazaa" in different contexts
    
    ### üîç What to Look For:
    - Strong attention (0.7+) = confident approximation
    - Distributed attention = considering multiple possibilities
    - Different heads = different approximation strategies
    """)
    
    # Add some examples of what to expect
    st.markdown("""
    ### üéØ Expected Patterns:
    - **"cat" ‚Üí "Katze"**: High direct attention
    - **"the" ‚Üí "Die"**: May attend to multiple words for grammar
    - **Verbs**: Often attend to subjects for agreement
    """)

# Footer
st.markdown("---")
st.markdown("""
### üöÄ About "Andaazaa Apna Apna"
This visualizer demonstrates how transformer models make their "Andaazaa" (approximations) when translating. 
Just like humans make educated guesses based on context, transformers use attention to approximate the best translations!

**Model**: Helsinki-NLP/opus-mt-en-de (English to German translation)
**Architecture**: Transformer with 6 layers, 8 attention heads each

*Andaazaa (ÿßŸÜÿØÿßÿ≤€Å) = Approximation/Estimate in Urdu/Hindi* üéØ
""")
